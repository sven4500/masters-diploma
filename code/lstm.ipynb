{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from scipy.sparse import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_cell:\n",
    "\n",
    "    def __init__(self, sess, input_dim, hidden_dim, learn_rate, optimizer='adagrad'):\n",
    "        self.sess = sess\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #self.ct_state = np.random.normal(size=[1, seq_dim])\n",
    "        #self.ht_state = np.random.normal(size=[1, seq_dim])\n",
    "        \n",
    "        # Начальное скрытое состояние ячейки.\n",
    "        self.ct_state = np.zeros([1, hidden_dim])\n",
    "        self.ht_state = np.zeros([1, hidden_dim])\n",
    "        \n",
    "        # Пара значений вход x и желаемый выход y.\n",
    "        self.x = tf.placeholder(tf.float32, [1, input_dim])\n",
    "        self.y = tf.placeholder(tf.float32, [1, input_dim])\n",
    "        \n",
    "        # Состояние сети. ht выходное значение, ct ячейка памяти.\n",
    "        self.ct = tf.placeholder(tf.float32, [1, hidden_dim])\n",
    "        self.ht = tf.placeholder(tf.float32, [1, hidden_dim])\n",
    "        \n",
    "        self.ft_wx,  self.ft_wh,  self.ft_b  = self.three() # Фильтр забывания (forget gate).\n",
    "        self.it_wx,  self.it_wh,  self.it_b  = self.three() # Фильтр входа (input gate).\n",
    "        self.ctt_wx, self.ctt_wh, self.ctt_b = self.three() # Фильтр кандидатов (candidate gate).\n",
    "        self.ot_wx,  self.ot_wh,  self.ot_b  = self.three() # Фильтр выхода (output gate).\n",
    "        \n",
    "        self.ft = tf.math.sigmoid(tf.matmul(self.x, self.ft_wx) + tf.matmul(self.ht, self.ft_wh) + self.ft_b)\n",
    "        self.it = tf.math.sigmoid(tf.matmul(self.x, self.it_wx) + tf.matmul(self.ht, self.it_wh) + self.it_b)\n",
    "        self.ctt = tf.math.tanh(tf.matmul(self.x, self.ctt_wx) + tf.matmul(self.ht, self.ctt_wh) + self.ctt_b)\n",
    "        self.ot = tf.math.sigmoid(tf.matmul(self.x, self.ot_wx) + tf.matmul(self.ht, self.ot_wh) + self.ot_b)\n",
    "        \n",
    "        self.ct_out = tf.math.multiply(self.ft, self.ct) + tf.math.multiply(self.it, self.ctt)\n",
    "        self.ht_out = tf.math.multiply(self.ot, tf.math.tanh(self.ct_out))\n",
    "        \n",
    "        self.out_w = tf.Variable(tf.random.normal([hidden_dim, input_dim]), dtype=tf.float32)\n",
    "        self.out_b = tf.Variable(tf.zeros([1, input_dim]), dtype=tf.float32)\n",
    "        self.out = tf.matmul(self.ht_out, self.out_w) + self.out_b\n",
    "        \n",
    "        self.cost = tf.reduce_mean(tf.square(self.out - self.y))\n",
    "        \n",
    "        if optimizer == 'adagrad':\n",
    "            self.train_op = tf.train.AdagradOptimizer(learn_rate).minimize(self.cost)\n",
    "        elif optimizer == 'adam':\n",
    "            self.train_op = tf.train.AdamOptimizer(learn_rate).minimize(self.cost)\n",
    "        elif optimizer == 'gradientdescent':\n",
    "            self.train_op = tf.train.GradientDescentOptimizer(learn_rate).minimize(self.cost)\n",
    "        elif optimizer == 'adadelta':\n",
    "            self.train_op = tf.train.AdadeltaOptimizer(learn_rate).minimize(self.cost)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Метод производит сохранение полного состояния ячейки включая матрицы весов.\n",
    "    def save_all(self):\n",
    "        \n",
    "        self.saved_ct_state = self.ct_state\n",
    "        self.saved_ht_state = self.ht_state\n",
    "        \n",
    "        self.saved_ft_wx = self.sess.run(self.ft_wx)\n",
    "        self.saved_ft_wh = self.sess.run(self.ft_wh)\n",
    "        \n",
    "        self.saved_it_wx = self.sess.run(self.it_wx)\n",
    "        self.saved_it_wh = self.sess.run(self.it_wh)\n",
    "        \n",
    "        self.saved_ctt_wx = self.sess.run(self.ctt_wx)\n",
    "        self.saved_ctt_wh = self.sess.run(self.ctt_wh)\n",
    "        \n",
    "        self.saved_ot_wx = self.sess.run(self.ot_wx)\n",
    "        self.saved_ot_wh = self.sess.run(self.ot_wh)\n",
    "    \n",
    "    # Метод производит восстановление ранее сохранённого состояния ячейки включая матрицы весов.\n",
    "    def restore_all(self):\n",
    "        \n",
    "        self.ct_state = self.saved_ct_state\n",
    "        self.ht_state = self.saved_ht_state\n",
    "        \n",
    "        self.sess.run(tf.assign(self.ft_wx, self.saved_ft_wx))\n",
    "        self.sess.run(tf.assign(self.ft_wh, self.saved_ft_wh))\n",
    "        \n",
    "        self.sess.run(tf.assign(self.it_wx, self.saved_it_wx))\n",
    "        self.sess.run(tf.assign(self.it_wh, self.saved_it_wh))\n",
    "        \n",
    "        self.sess.run(tf.assign(self.ctt_wx, self.saved_ctt_wx))\n",
    "        self.sess.run(tf.assign(self.ctt_wh, self.saved_ctt_wh))\n",
    "        \n",
    "        self.sess.run(tf.assign(self.ot_wx, self.saved_ot_wx))\n",
    "        self.sess.run(tf.assign(self.ot_wh, self.saved_ot_wh))\n",
    "        \n",
    "    # Метод производит SVD сжатие слоёв ячейки. Можно установить различные факторы сжатия\n",
    "    # для входного и скрытого слоёв.\n",
    "    def svd_compress(self, factor_input, factor_hidden):\n",
    "\n",
    "        # Делаем SVD разложение для каждого слоя.\n",
    "        ft_wx_compr, ft_wx_norm, _, ft_wx_size = self.svd_compress_one(self.ft_wx, factor_input)\n",
    "        ft_wh_compr, ft_wh_norm, _, ft_wh_size = self.svd_compress_one(self.ft_wh, factor_hidden)\n",
    "        \n",
    "        it_wx_compr, it_wx_norm, _, it_wx_size = self.svd_compress_one(self.it_wx, factor_input)\n",
    "        it_wh_compr, it_wh_norm, _, it_wh_size = self.svd_compress_one(self.it_wh, factor_hidden)\n",
    "        \n",
    "        ctt_wx_compr, ctt_wx_norm, _, ctt_wx_size = self.svd_compress_one(self.ctt_wx, factor_input)\n",
    "        ctt_wh_compr, ctt_wh_norm, _, ctt_wh_size = self.svd_compress_one(self.ctt_wh, factor_hidden)\n",
    "        \n",
    "        ot_wx_compr, ot_wx_norm, _, ot_wx_size = self.svd_compress_one(self.ot_wx, factor_input)\n",
    "        ot_wh_compr, ot_wh_norm, _, ot_wh_size = self.svd_compress_one(self.ot_wh, factor_hidden)\n",
    "        \n",
    "        # Заменяем весовые коэффициенты данными после SVD разложения.\n",
    "        self.sess.run(tf.assign(self.ft_wx, ft_wx_compr))\n",
    "        self.sess.run(tf.assign(self.ft_wh, ft_wh_compr))\n",
    "        \n",
    "        self.sess.run(tf.assign(self.it_wx, it_wx_compr))\n",
    "        self.sess.run(tf.assign(self.it_wh, it_wh_compr))\n",
    "        \n",
    "        self.sess.run(tf.assign(self.ctt_wx, ctt_wx_compr))\n",
    "        self.sess.run(tf.assign(self.ctt_wh, ctt_wh_compr))\n",
    "        \n",
    "        self.sess.run(tf.assign(self.ot_wx, ot_wx_compr))\n",
    "        self.sess.run(tf.assign(self.ot_wh, ot_wh_compr))\n",
    "        \n",
    "        # Считаем среднюю норму всех матриц весов после сжатия.\n",
    "        norm = (ft_wx_norm + ft_wh_norm + it_wx_norm + it_wh_norm + ctt_wx_norm + ctt_wh_norm + ot_wx_norm + ot_wh_norm) / 8\n",
    "        size = ft_wx_size + ft_wh_size + it_wx_size + it_wh_size + ctt_wx_size + ctt_wh_size + ot_wx_size + ot_wh_size\n",
    "        \n",
    "        return norm, size\n",
    "\n",
    "    # Метод производит сингулярное разложение матрицы mat и оставляет только перых factor элементов матрицы.\n",
    "    def svd_compress_one(self, mat, factor):\n",
    "        \n",
    "        A = self.sess.run(mat)\n",
    "        #A = np.matrix([[1, 1, 1, 0, 0], [3, 3, 3, 0, 0], [4, 4, 4, 0, 0],\n",
    "            #[5, 5, 5, 0, 0], [0, 2, 0, 4, 4], [0, 0, 0, 5, 5], [0, 1, 0, 2, 2]], dtype=np.double)\n",
    "        m, n = A.shape\n",
    "        \n",
    "        # Вычисляем сколько сингулярных значений оставить.\n",
    "        k_top = int(min(m, n) * factor)\n",
    "        \n",
    "        U, s, Vt = scipy.sparse.linalg.svds(A, k=k_top)\n",
    "        #U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "        S = np.diag(s)\n",
    "        \n",
    "        U_m, U_n = U.shape\n",
    "        S_m, S_n = S.shape\n",
    "        Vt_m, Vt_n = Vt.shape\n",
    "        \n",
    "        # Считаем суммарное количество элементов после разложения.\n",
    "        size = U_m * U_n + S_m * S_n + Vt_m * Vt_n\n",
    "        \n",
    "        # Новая матрица с выкинутой чатью элементов. По размеру она совпадает с исходной матрицей.\n",
    "        # Тем не менее таким образом симулируется поведение прореживания матрицы.\n",
    "        B = np.dot(np.dot(U, S), Vt)\n",
    "        norm = self.frob_norm(A, B)\n",
    "        \n",
    "        return B, norm, k_top, size\n",
    "\n",
    "    # Норма Фробениуса здесь используется в качестве метрики схожести двух матриц.\n",
    "    # Используется нормализационный множитель 1/n где n количество элементов матрицы.\n",
    "    def frob_norm(self, A, B):\n",
    "        \n",
    "        assert A.shape == B.shape\n",
    "        m, n = A.shape\n",
    "        return np.sqrt(np.sum(np.square(A - B)) / (m * n))\n",
    "    \n",
    "    def clear_state(self):\n",
    "        \n",
    "        self.ct_state = np.zeros([1, self.hidden_dim])\n",
    "        self.ht_state = np.zeros([1, self.hidden_dim])\n",
    "    \n",
    "    def three(self):\n",
    "        # Для LSTM распространённая практика bias задавать как 1.\n",
    "        # https://datascience.stackexchange.com/questions/17987/how-should-the-bias-be-initialized-and-regularized\n",
    "        # Вектор входных значений переводим в скрытое состояние.\n",
    "        return (tf.Variable(tf.random.normal([input_dim, hidden_dim]), dtype=tf.float32),\n",
    "            tf.Variable(tf.random.normal([hidden_dim, hidden_dim]), dtype=tf.float32),\n",
    "            tf.Variable(tf.zeros([1, hidden_dim]), dtype=tf.float32)) # <- попробуем на нулевом смещении\n",
    "            #tf.Variable(tf.random.normal([1, hidden_dim]), dtype=tf.float32)) # <- пробуем на случайном смещении\n",
    "            #tf.Variable(tf.ones([1, hidden_dim]), dtype=tf.float32)) # <- пробуем все 1\n",
    "    \n",
    "    def train(self, train_x, train_y, repeat):\n",
    "                    \n",
    "        for i in range(repeat):\n",
    "            \n",
    "            cost, _ = self.sess.run([self.cost, self.train_op],\n",
    "                feed_dict={self.x:train_x, self.y:train_y, self.ct:self.ct_state, self.ht:self.ht_state})\n",
    "            \n",
    "        self.ct_state, self.ht_state = self.sess.run([self.ct_out, self.ht_out],\n",
    "            feed_dict={self.x:train_x, self.ct:self.ct_state, self.ht:self.ht_state})\n",
    "            \n",
    "        return cost\n",
    "    \n",
    "    def test(self, test_x):\n",
    "                \n",
    "        self.ct_state, self.ht_state, out = self.sess.run([self.ct_out, self.ht_out, self.out],\n",
    "            feed_dict={self.x:test_x, self.ct:self.ct_state, self.ht:self.ht_state})\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
